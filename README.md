# ğŸ¬ Predicting Movie Success with Machine Learning

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.18.0-orange.svg)
![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.2.2-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status](https://img.shields.io/badge/Status-Complete-success.svg)

**Can we predict if a movie will succeed? Turns out, we can â€” with 86.8% accuracy!**

[ğŸ“Š View Notebook](movie_success_prediction.ipynb) â€¢ [ğŸ“ˆ Results](#-results) â€¢ [ğŸš€ Quick Start](#-how-to-run)

</div>

---

## ğŸ“Œ What This Project Is About

Ever wonder what separates box office hits from flops? I built a machine learning system that predicts whether a movie will be successful by analyzing everything from budgets and ratings to the actual words used in movie descriptions.

Here's how I defined "success": A movie makes it if it either pulls in serious revenue (above the median) **OR** gets great reviews (rating > 6.5/10). Because let's face it â€” critical darlings and commercial blockbusters are both winners in their own right.

### ğŸ¯ The Big Win
> My **Random Forest model achieved 86.8% F1-Score** by combining:
> - Structured data like budget, popularity, and ratings
> - Text analysis on movie descriptions using NLP
> - 114 carefully engineered features that capture what makes movies tick

---

## ğŸ¯ What I Set Out to Do

<table>
<tr>
<td width="50%">

### Business Goals
- ğŸ¯ Predict whether a movie will succeed before it even releases
- ğŸ’° Give studios data-driven insights for smarter investments
- ğŸ“Š Figure out what actually drives success
- ğŸ¬ Help optimize marketing strategies

</td>
<td width="50%">

### Technical Goals
- ğŸ¤– Test and compare different ML algorithms to find the best one
- ğŸ“ Apply natural language processing to movie descriptions
- ğŸ”§ Engineer features that actually matter
- ğŸ“ˆ Beat the 80% accuracy threshold

</td>
</tr>
</table>

---

## ğŸ”„ How I Built This Thing

The workflow follows the classic machine learning pipeline, but with some NLP magic thrown in:

```mermaid
graph LR
    A[ğŸ“¥ Data Collection] --> B[ğŸ” EDA]
    B --> C[ğŸ“ NLP Analysis]
    C --> D[ğŸ”§ Preprocessing]
    D --> E[ğŸ¤– Model Training]
    E --> F[ğŸ“Š Evaluation]
    F --> G[ğŸ† Best Model]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#f0e1ff
    style D fill:#e1ffe1
    style E fill:#ffe1e1
    style F fill:#fff0e1
    style G fill:#90EE90
```

### Step-by-Step Breakdown

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1ï¸âƒ£ DATA COLLECTION                          â”‚
â”‚  Got my hands on two datasets:                                 â”‚
â”‚  â€¢ TMDB (4,803 movies with all the metadata)                   â”‚
â”‚  â€¢ Wikipedia (34,000+ plot summaries for text analysis)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2ï¸âƒ£ EXPLORATORY DATA ANALYSIS (EDA)                â”‚
â”‚  Dove deep into the data to understand patterns:               â”‚
â”‚  â€¢ What's missing? How are revenues distributed?               â”‚
â”‚  â€¢ Which features correlate with success?                      â”‚
â”‚  â€¢ Visualized everything to spot the story in the data         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 2.5ï¸âƒ£ NLP ANALYSIS (The Fun Part)               â”‚
â”‚  Analyzed 34,000+ movie plots to extract linguistic patterns:  â”‚
â”‚  â€¢ Cleaned and preprocessed text                               â”‚
â”‚  â€¢ Generated word clouds (what words predict success?)         â”‚
â”‚  â€¢ Sentiment analysis (optimistic vs dark themes)              â”‚
â”‚  â€¢ Extracted the 100 most important words using TF-IDF         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            3ï¸âƒ£ FEATURE ENGINEERING & PREPROCESSING              â”‚
â”‚  Built 114 features from raw data:                             â”‚
â”‚  â€¢ 4 core numbers (budget, popularity, votes, runtime)         â”‚
â”‚  â€¢ 100 text features from TF-IDF                               â”‚
â”‚  â€¢ 10 custom NLP features (sentiment, complexity, etc.)        â”‚
â”‚  â€¢ Scaled everything and split 80-20 for training/testing      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4ï¸âƒ£ MODEL TRAINING (The Showdown)                  â”‚
â”‚  Trained 4 different algorithms to see which wins:             â”‚
â”‚  ğŸŒ² Random Forest  ğŸ“ˆ Logistic Regression                       â”‚
â”‚  ğŸ§  Neural Network  ğŸ” SVM                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   5ï¸âƒ£ EVALUATION & COMPARISON                   â”‚
â”‚  Compared all models using:                                    â”‚
â”‚  â€¢ Confusion matrices â€¢ Accuracy & F1-Scores                   â”‚
â”‚  â€¢ Feature importance â€¢ Visualization charts                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             ğŸ† AND THE WINNER IS... RANDOM FOREST! ğŸ†           â”‚
â”‚              Accuracy: 82.9% | F1-Score: 86.8%                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š The Data Sources

I used **2 different datasets** to build this predictor:

### 1. TMDB Movie Metadata (The Main Dataset)
**Source:** [TMDB on Kaggle](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata)

This is where the model gets trained and tested. It's got:
- **4,803 movies** with rich metadata
- **20 features** including budget, revenue, popularity, ratings, vote counts, runtime, and plot descriptions
- **Purpose:** Train the models and make predictions
- **Target Variable:** Success or Not Success (binary classification)
- **Interesting fact:** 65.2% of movies in this dataset are "successful" by our definition

### 2. Wikipedia Movie Plots (The Text Analysis Dataset)
**Source:** [Wikipedia Movie Plots on Kaggle](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots)

This massive dataset helped me understand patterns in storytelling:
- **34,000+ movies** with full plot summaries
- **7 columns** covering release year, title, director, cast, genre, and detailed plots
- **Purpose:** NLP exploration, word frequency analysis, sentiment extraction, and word clouds
- **How I used it:** Extracted linguistic patterns to engineer better text features for the TMDB data

### What Features Did I Actually Use?

After all the engineering, I ended up with **114 features**:
- **4 Numerical Features:** Budget, Popularity Score, Vote Count, Runtime
- **100 Text Features:** Top keywords extracted using TF-IDF from movie descriptions
- **10 NLP Features:** Sentiment scores, text complexity metrics, emotional tone, and more

---

## ğŸ”¬ The Process (How I Actually Did This)

### 1. **Exploratory Data Analysis (EDA)**
First things first â€” I needed to understand what I was working with:
- Checked for missing data and figured out how to handle it
- Looked at how revenue, budgets, and ratings are distributed
- Built correlation heatmaps to see what features relate to success
- Created visualizations to spot patterns that numbers alone wouldn't show

### 2. **Natural Language Processing (NLP)**
This is where it got interesting. I analyzed text from 34,000+ movie plots:
- **Text preprocessing:** Cleaned up the messy plot summaries
- **Word frequency analysis:** What words appear most in successful vs unsuccessful movies?
- **N-grams:** Found common phrases (bigrams and trigrams) that predict success
- **Word clouds:** Visual representations showing the vocabulary of hits vs flops
- **TF-IDF vectorization:** Extracted the 100 most important words from movie descriptions
- **Sentiment analysis:** Used TextBlob to measure optimism/pessimism and subjectivity
- **Text complexity:** Measured vocabulary richness, sentence structure, and readability
- **Feature engineering:** Created 10 custom NLP features that capture linguistic patterns

### 3. **Data Preprocessing**
Had to get the data ready for the models:
- Filled in missing values (median for numbers, empty strings for text)
- Scaled all features using StandardScaler so no single feature dominates
- Split the data 80-20 (training vs testing) with stratification to keep class balance

### 4. **Model Training & Evaluation**

I trained 4 different algorithms and let them compete:

| Model | Accuracy | F1-Score | Training Speed | My Take |
|-------|----------|----------|----------------|----------|
| **ğŸŒ² Random Forest** ğŸ† | **82.9%** | **86.8%** | Fast âš¡ | **The winner!** Great balance of speed and accuracy |
| ğŸ“ˆ Logistic Regression | 81.1% | 85.3% | Very Fast âš¡âš¡ | Surprisingly good for a simple linear model |
| ğŸ§  Neural Network | 77.9% | 83.2% | Slow ğŸŒ | Took forever to train, didn't beat simpler models |
| ğŸ” SVM | 76.4% | 82.4% | Medium âš¡ | Decent but Random Forest crushes it |

---

## ğŸ† Results (The Good Stuff)

<div align="center">

### ğŸ¯ Model Performance Showdown

| Model | Accuracy | Precision | Recall | F1-Score | Speed |
|-------|----------|-----------|--------|----------|-------|
| **ğŸŒ² Random Forest** | **82.9%** | **85.2%** | **88.5%** | **86.8%** â­ | âš¡âš¡âš¡ |
| ğŸ“ˆ Logistic Regression | 81.1% | 83.7% | 87.0% | 85.3% | âš¡âš¡âš¡âš¡ |
| ğŸ§  Neural Network | 77.9% | 81.4% | 85.1% | 83.2% | âš¡ |
| ğŸ” SVM | 76.4% | 79.8% | 84.5% | 82.4% | âš¡âš¡ |

</div>

### ğŸ… The Champion: Random Forest Classifier

Random Forest came out on top, and here's why it won:

<table>
<tr>
<td width="50%">

#### ğŸ“Š Performance Metrics
- âœ… **Accuracy:** 82.9% â€” Got it right 829 times out of 1,000
- âœ… **F1-Score:** 86.8% â€” Best overall balance (this is what matters!)
- âœ… **Precision:** 85.2% â€” When it says "success," it's right 85% of the time
- âœ… **Recall:** 88.5% â€” Catches 88.5% of all successful movies
- âœ… **Training Time:** ~600ms â€” Blazing fast!
- âœ… **Configuration:** 100 decision trees, max depth of 10

</td>
<td width="50%">

#### ğŸ¯ What It Got Right (and Wrong)
```
                Predicted
              Success  Fail
Actual Success   539    88
       Fail       76   258
```
Breaking it down:
- **539** True Positives (correctly predicted success âœ…)
- **258** True Negatives (correctly predicted failure âœ…)
- **76** False Positives (thought it would succeed but didn't âŒ)
- **88** False Negatives (missed some successes âŒ)

</td>
</tr>
</table>

### ğŸ“ˆ What Actually Matters? (Feature Importance)

Here are the top 10 features the model relies on:

```
1. ğŸ¥‡ Vote Count     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 26.5%
2. ğŸ¥ˆ Popularity     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  25.7%
3. ğŸ¥‰ Budget         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           16.8%
4.    Runtime        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     7.8%
5.    world          â–ˆâ–ˆâ–ˆâ–ˆ                        4.9%
6.    wife           â–ˆâ–ˆâ–ˆâ–ˆ                        4.9%
7.    group          â–ˆâ–ˆâ–ˆâ–ˆ                        4.9%
8.    men            â–ˆâ–ˆâ–ˆâ–ˆ                        4.4%
9.    life           â–ˆâ–ˆâ–ˆâ–ˆ                        4.2%
10.   set            â–ˆâ–ˆâ–ˆâ–ˆ                        4.2%
```

**Big takeaway:** The top 3 features alone (vote count, popularity, budget) account for **69%** of the model's prediction power! 

Turns out audience engagement (votes), pre-release buzz (popularity), and studio investment (budget) are the holy trinity of movie success prediction.

---

## ğŸ’¡ What I Learned (Key Insights)

<table>
<tr>
<td width="33%">

### ğŸ“Š About the Data
- ğŸ“ˆ **65.2%** of movies are successful (more winners than losers!)
- ğŸ’° Median revenue: **$19.2M**
- â­ Critical threshold for "success": **6.5/10** rating
- ğŸ“ Average description length: **52 words**
- ğŸ¬ Total engineered features: **114**

</td>
<td width="33%">

### ğŸ” About Features
- ğŸ¥‡ Vote count is the **single strongest** predictor
- ğŸ’¡ Just popularity + budget account for **69%** of prediction power
- ğŸ“ Adding text features **boosted accuracy by 8%**
- ğŸ¯ NLP features (sentiment, complexity) add meaningful signal
- âš–ï¸ Feature scaling was **crucial** â€” without it, models performed poorly

</td>
<td width="33%">

### ğŸ¤– About Models
- ğŸŒ² Ensemble methods beat fancy neural networks
- ğŸ¯ F1-Score is more important than raw accuracy for imbalanced data
- âš¡ Random Forest = sweet spot of speed + accuracy
- ğŸ§  Neural networks took 10x longer but performed worse
- ğŸ“Š Even the "worst" model hit **75%+ accuracy**

</td>
</tr>
</table>

### ğŸ¬ What This Means for the Business

| What I Found | Why It Matters | What Studios Should Do |
|---------|--------|----------------|
| ğŸ¯ **Vote count is king** | More engagement = higher success rate | Focus on building an audience *before* release day |
| ğŸ’° **Budget correlates with success** | Bigger investments tend to pay off | Allocate budgets strategically to projects with potential |
| â­ **Quality beats quantity** | Critical acclaim is a valid path to success | Don't ignore artistic quality in favor of commercial appeal |
| ğŸ“ **Marketing copy matters** | How you describe a movie affects interest | Invest in compelling, well-crafted descriptions |
| ğŸ­ **There's more than one path to success** | You can win with revenue OR ratings | Don't chase only box office â€” critical darlings count too |

---

## ğŸ› ï¸ Tech Stack

<div align="center">

### Core Technologies

| Category | Technologies |
|----------|-------------|
| **Language** | ![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white) |
| **Data Processing** | ![Pandas](https://img.shields.io/badge/Pandas-2.2.3-150458?style=for-the-badge&logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/NumPy-1.26.4-013243?style=for-the-badge&logo=numpy&logoColor=white) |
| **Machine Learning** | ![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.2.2-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white) |
| **Deep Learning** | ![TensorFlow](https://img.shields.io/badge/TensorFlow-2.18.0-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white) ![Keras](https://img.shields.io/badge/Keras-D00000?style=for-the-badge&logo=keras&logoColor=white) |
| **NLP** | ![TextBlob](https://img.shields.io/badge/TextBlob-0.17.1-4B8BBE?style=for-the-badge) |
| **Visualization** | ![Matplotlib](https://img.shields.io/badge/Matplotlib-3.8.2-11557c?style=for-the-badge) ![Seaborn](https://img.shields.io/badge/Seaborn-0.13.0-3776AB?style=for-the-badge) |
| **Platform** | ![Kaggle](https://img.shields.io/badge/Kaggle-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white) ![VS Code](https://img.shields.io/badge/VS_Code-007ACC?style=for-the-badge&logo=visual-studio-code&logoColor=white) |
| **Hardware** | ![GPU](https://img.shields.io/badge/2x_Tesla_T4-76B900?style=for-the-badge&logo=nvidia&logoColor=white) ![CUDA](https://img.shields.io/badge/CUDA-12.4-76B900?style=for-the-badge&logo=nvidia&logoColor=white) |

</div>

### ğŸ“š Libraries & Frameworks

```python
# Data Processing
pandas==2.2.3
numpy==1.26.4

# Machine Learning
scikit-learn==1.2.2
xgboost (optional)

# Deep Learning
tensorflow==2.18.0
keras (included in TensorFlow)

# NLP & Text Processing
textblob==0.17.1
wordcloud==1.9.3

# Visualization
matplotlib==3.8.2
seaborn==0.13.0

# Environment
jupyter
notebook
```

### ğŸ–¥ï¸ Development Environment

```
Platform:    Kaggle Notebooks
CPU:         4 cores
GPU:         2x NVIDIA Tesla T4 (16GB each)
CUDA:        12.4
Python:      3.11.13
OS:          Linux-6.6.56+
IDE:         VS Code with Jupyter Extension
```

---

## ğŸ“ Project Structure

```
ML-Project/
â”‚
â”œâ”€â”€ ğŸ““ movie_success_prediction.ipynb    # Main notebook (all phases)
â”‚   â”œâ”€â”€ Phase 1: Setup & Data Loading
â”‚   â”œâ”€â”€ Phase 2: Exploratory Data Analysis
â”‚   â”œâ”€â”€ Phase 2.5: NLP Analysis (13 sections)
â”‚   â”œâ”€â”€ Phase 3: Preprocessing (7 sections)
â”‚   â”œâ”€â”€ Phase 4: Model Training (4 models)
â”‚   â”œâ”€â”€ Phase 5: Model Comparison
â”‚   â””â”€â”€ Phase 6: Conclusions & Insights
â”‚
â”œâ”€â”€ ğŸ“„ README.md                         # This file
â”‚
â”œâ”€â”€ ğŸ“Š data/ (not included in repo)
â”‚   â”œâ”€â”€ tmdb_5000_movies.csv            # TMDB dataset (4,803 movies)
â”‚   â””â”€â”€ wiki_movie_plots_deduped.csv    # Wikipedia plots (34,000+)
â”‚
â”œâ”€â”€ ğŸ“ˆ outputs/ (generated at runtime)
â”‚   â”œâ”€â”€ visualizations/
â”‚   â”‚   â”œâ”€â”€ confusion_matrices.png
â”‚   â”‚   â”œâ”€â”€ model_comparison.png
â”‚   â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”‚   â””â”€â”€ eda_plots/
â”‚   â”‚       â”œâ”€â”€ revenue_distribution.png
â”‚   â”‚       â”œâ”€â”€ correlation_heatmap.png
â”‚   â”‚       â””â”€â”€ success_analysis.png
â”‚   â”‚
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ model_results.csv
â”‚       â””â”€â”€ project_summary.txt
â”‚
â””â”€â”€ ğŸ“‹ requirements.txt (optional)
```

### ğŸ“Š Notebook Structure

```mermaid
graph TD
    A[ğŸ““ Main Notebook] --> B[Phase 1: Setup]
    A --> C[Phase 2: EDA]
    A --> D[Phase 2.5: NLP]
    A --> E[Phase 3: Preprocessing]
    A --> F[Phase 4: Training]
    A --> G[Phase 5: Comparison]
    A --> H[Phase 6: Insights]
    
    D --> D1[2.5.1-2.5.8: Text Analysis]
    D --> D2[2.5.9: Sentiment]
    D --> D3[2.5.10: Complexity]
    D --> D4[2.5.11-2.5.13: Engineering]
    
    F --> F1[Random Forest]
    F --> F2[SVM]
    F --> F3[Logistic Reg]
    F --> F4[Neural Net]
```

---

## ğŸš€ Want to Run This Yourself?

### 1. Clone the Repository
```bash
git clone https://github.com/laxmikhilnani/ML-Project.git
cd ML-Project
```

### 2. Install What You Need
All the required libraries in one command:
```bash
pip install pandas numpy matplotlib seaborn scikit-learn tensorflow textblob wordcloud
```

### 3. Download the Datasets (Both Required!)
You need **both datasets** from Kaggle. Here's where to get them:

**Dataset 1: TMDB Movie Metadata** (The main training data)
- URL: [https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata)
- Download: `tmdb_5000_movies.csv`

**Dataset 2: Wikipedia Movie Plots** (For NLP analysis)
- URL: [https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots)
- Download: `wiki_movie_plots_deduped.csv`

Drop both CSV files in the project folder, or update the paths in the notebook if you put them somewhere else.

### 4. Fire It Up!
**Option A:** Classic Jupyter
```bash
jupyter notebook movie_success_prediction.ipynb
```

**Option B:** VS Code (my preference)
- Open the project in VS Code
- Install the Jupyter extension if you haven't already
- Open the notebook
- Run all cells (or step through them one by one)

The whole thing takes about 10-15 minutes to run. Neural network training is the slowest part.

---

## ğŸ“ˆ Visualizations

<div align="center">

### ğŸ¨ Visual Analytics Included

</div>

<table>
<tr>
<td width="50%">

#### ğŸ“Š EDA Visualizations
- ğŸ“ˆ Revenue distribution histograms
- ğŸ’° Budget vs. Revenue scatter plots
- â­ Rating distribution analysis
- ğŸ”¥ Correlation heatmaps
- ğŸ“‰ Missing value analysis
- ğŸ¯ Success rate pie charts

</td>
<td width="50%">

#### ğŸ“ NLP Visualizations
- â˜ï¸ Word clouds (success vs. failure)
- ğŸ“ Text length distributions
- ğŸ”¤ N-gram frequency charts
- ğŸ˜Š Sentiment analysis plots
- ğŸ“– Complexity metrics
- ğŸ­ Text comparison charts

</td>
</tr>
<tr>
<td width="50%">

#### ğŸ¤– Model Visualizations
- ğŸ¯ Confusion matrices (4 models)
- ğŸ“Š Performance comparison bars
- ğŸŒŸ Polar/radar charts
- ğŸ“ˆ Training history (Neural Net)
- ğŸ† F1-Score comparisons
- âš¡ Speed vs. accuracy trade-offs

</td>
<td width="50%">

#### ğŸ” Feature Visualizations
- ğŸ“Š Feature importance bars
- ğŸ¯ Top 15 features chart
- ğŸ”— Feature correlation matrix
- ğŸ“ˆ NLP feature correlations
- ğŸ’¡ Numerical vs. Text importance
- ğŸŒ² Random Forest feature weights

</td>
</tr>
</table>

### ğŸ“¸ Sample Visualizations

```
ğŸ¬ Movie Success Prediction - Visual Gallery
â”œâ”€â”€ ğŸ“Š EDA Phase
â”‚   â”œâ”€â”€ Revenue Distribution          [Histogram]
â”‚   â”œâ”€â”€ Budget vs Revenue             [Scatter Plot]
â”‚   â””â”€â”€ Feature Correlations          [Heatmap]
â”‚
â”œâ”€â”€ ğŸ“ NLP Phase
â”‚   â”œâ”€â”€ Success Word Cloud            [Word Cloud - Green]
â”‚   â”œâ”€â”€ Failure Word Cloud            [Word Cloud - Red]
â”‚   â””â”€â”€ Sentiment Analysis            [Distribution]
â”‚
â”œâ”€â”€ ğŸ¤– Model Phase
â”‚   â”œâ”€â”€ 4 Confusion Matrices          [Heatmaps]
â”‚   â”œâ”€â”€ Model Comparison              [Bar Chart]
â”‚   â””â”€â”€ Performance Radar             [Polar Plot]
â”‚
â””â”€â”€ ğŸ¯ Insights Phase
    â”œâ”€â”€ Feature Importance            [Horizontal Bars]
    â””â”€â”€ Final Summary                 [Dashboard]
```

---

## ğŸ”® What's Next? (Future Improvements)

There's always room to make this better. Here's what I'm thinking:

### Model Improvements
- [ ] Fine-tune hyperparameters with GridSearchCV (squeeze out every last % of accuracy)
- [ ] Try XGBoost and LightGBM (they're supposedly faster and better)
- [ ] Stack multiple models together (ensemble of ensembles!)
- [ ] Add k-fold cross-validation for more robust evaluation

### Feature Engineering
- [ ] Add cast and crew data (does having A-list actors help?)
- [ ] Include genre information (do horror films follow different patterns than romances?)
- [ ] Factor in release timing (summer blockbusters vs Oscar season)
- [ ] Scrape social media sentiment before release

### Additional Analysis
- [ ] Predict regional performance (what works in Asia vs North America?)
- [ ] Build genre-specific models (action films might need their own predictor)
- [ ] Predict ROI instead of just success (how much bang for your buck?)
- [ ] Time series analysis (have success patterns changed over the decades?)

---

## ğŸ“ The Bottom Line

Here's what this project proves:

1. **ML actually works for real-world problems** â€” We hit 86.8% F1-score predicting movie success. That's useful!

2. **Combining data types is powerful** â€” Mixing structured data (budgets, ratings) with unstructured text (descriptions) beats using either alone

3. **Sometimes simple wins** â€” Random Forest (a straightforward ensemble method) crushed the fancy neural network

4. **Feature engineering is where the magic happens** â€” Spending time creating good features matters more than picking the "best" algorithm

5. **This has actual business value** â€” Studios could use this to make smarter investment decisions and optimize marketing spend

---

## ğŸ‘¨â€ğŸ’» About Me

**Laxmi Khilnani**
- GitHub: [@laxmikhilnani20](https://github.com/laxmikhilnani20)
- Project: Movie Success Prediction using ML & NLP

---

## ğŸ“„ License

This project is open-source and available for educational purposes. Feel free to use it, learn from it, or build on it!

---

## ğŸ™ Credit Where It's Due

Big thanks to:
- **TMDB** for the comprehensive movie metadata dataset
- **Wikipedia** for 34,000+ movie plot summaries
- **Kaggle** for hosting the datasets and providing free GPU resources (2x Tesla T4 GPUs!)
- **Scikit-learn** community for building amazing ML libraries
- **TensorFlow team** for the deep learning framework
- **TextBlob** for making NLP accessible

---

## ğŸ“§ Questions? Feedback?

Got questions or want to chat about this project?
- ğŸ’¬ Open an issue on GitHub
- ğŸ› Found a bug? Let me know!
- ğŸ’¡ Have ideas for improvements? I'm all ears
- ğŸ“§ Connect via my GitHub profile

---

**â­ If you found this useful or learned something, drop a star! It genuinely makes my day.**

---

## ğŸ“Š Project By the Numbers

<div align="center">

| Metric | Value |
|--------|-------|
| ğŸ“ Lines of Code | 2,500+ |
| ğŸ““ Notebook Cells | 107 |
| ğŸ“Š Visualizations | 25+ charts and plots |
| ğŸ¤– Models Trained | 4 different algorithms |
| ğŸ“ˆ Features | 114 engineered features |
| â±ï¸ Runtime | ~10-15 minutes |
| ğŸ¯ Best F1-Score | **86.8%** (Random Forest) |
| ğŸ“š Datasets | 2 (TMDB + Wikipedia) |
| ğŸ¬ Movies Analyzed | 4,803 for training + 34,000+ for NLP |

</div>

---

## ğŸ¤ Want to Contribute?

I'd love to see what you can add! Here's how:

1. ğŸ´ Fork this repo
2. ğŸŒ¿ Create a branch for your feature (`git checkout -b feature/CoolNewThing`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Added cool new thing'`)
4. ğŸ“¤ Push it up (`git push origin feature/CoolNewThing`)
5. ğŸ”€ Open a Pull Request and let's chat!

---

<div align="center">

**Built with â¤ï¸, coffee â˜•, and a lot of trial and error**

*Last Updated: November 2025*

[![GitHub](https://img.shields.io/badge/GitHub-laxmikhilnani20-181717?style=for-the-badge&logo=github)](https://github.com/laxmikhilnani20)

</div>
# ML_project
